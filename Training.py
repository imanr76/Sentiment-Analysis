import torchfrom torch import nnfrom torch.utils.data import DataLoader, Datasetclass dataset(Dataset):    def __init__(self,data):        self.data = data        def __len__(self):        return len(self.data)        def __getitem__(self, index):        return self.data[index]if torch.backends.mps.is_available():   mps_device = torch.device("mps")vocabulary = torch.load("./models/vocabulary.pth")training_dataset = torch.load("./data/training/training_dataset.pth")validation_dataset = torch.load("./data/validation/validation_dataset.pth")test_dataset = torch.load("./data/test/test_dataset.pth")train_loader = DataLoader(training_dataset, batch_size = 256, shuffle = True)validation_loader = DataLoader(validation_dataset, batch_size = 256, shuffle = True)test_loader = DataLoader(test_dataset, batch_size = 256, shuffle = True)#%%class sentiment(nn.Module):        def __init__(self, vocab_len, embed_dim = 300, lstm_size = 50, bidirectional = True):        super().__init__()        self.bidirectional = bidirectional                self.embeddings = nn.Embedding(vocab_len, embed_dim)        self.lstm = nn.LSTM(embed_dim, hidden_size = lstm_size, batch_first = True, bidirectional = bidirectional)                if self.bidirectional:            self.linear_size = lstm_size * 2        else:            self.linear_size = lstm_size                    self.linear =nn.Linear(self.linear_size, 3)        self.relu = nn.ReLU()        self.softmax = nn.Softmax(dim = 1)        def forward(self, inputs):        x = self.embeddings(inputs)        x = self.lstm(x)[1][0]                if self.bidirectional:            x = x.permute(1,0,2).reshape(x.size(1),-1)        else:            x = x.reshape(x.size(1),-1)        x = self.linear(x)        x = self.softmax(x)        return x        def train(self, train_dataloader, validation_dataset, epochs, loss_func, optimizer):        loss_train = []                for epoch in range(epochs):            for data in train_dataloader:                data = data.to(mps_device)                review = data[:, :-1]                sentiment_real = data[:, -1]                                sentiment_pred = self.forward(review)                optimizer.zero_grad()                loss_train = loss_func(sentiment_pred, sentiment_real)                loss_train.backward()                            print(f"training loss : {loss_train:.4f}")            #print(f"validation loss : {loss_func(self.forward(validation_dataset[:, :-1]), validation_dataset[:, -1]):.4f}")            print()        def evaluate(self):        pass    import timestart_time = time.time()torch.mps.synchronize()# mps_device = 'cpu'mps_device = torch.device("mps")model = sentiment(len(vocabulary)).to(mps_device)loss_func = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr = 0.0000001)model.train(train_loader, validation_dataset, 10, loss_func, optimizer )elapsed_time = time.time() - start_timeprint( "Time: ", elapsed_time)