import torchfrom torch import nnfrom torch.utils.data import DataLoader, Datasetimport matplotlib.pyplot as pltfrom sklearn.metrics import classification_report# set KMP_DUPLICATE_LIB_OK=Trueclass dataset(Dataset):    def __init__(self,data):        self.data = data        def __len__(self):        return len(self.data)        def __getitem__(self, index):        return self.data[index]if torch.cuda.is_available():   device = torch.device("cuda")vocabulary = torch.load("./models/vocabulary.pth")training_dataset = torch.load("./data/training/training_dataset.pth")validation_dataset = torch.load("./data/validation/validation_dataset.pth")test_dataset = torch.load("./data/test/test_dataset.pth")train_loader = DataLoader(training_dataset, batch_size = 256, shuffle = True)validation_loader = DataLoader(validation_dataset, batch_size = 256, shuffle = True)test_loader = DataLoader(test_dataset, batch_size = 256, shuffle = True)#%%class sentiment(nn.Module):        def __init__(self, vocab_len, embed_dim = 20, lstm_size = 10, bidirectional = True, num_layers = 1):        super().__init__()        self.num_layers = num_layers        self.bidirectional = bidirectional                self.embeddings = nn.Embedding(vocab_len, embed_dim)        self.lstm = nn.LSTM(embed_dim, hidden_size = lstm_size, batch_first = True, bidirectional = bidirectional, dropout = 0.0)                if self.bidirectional:            self.linear_size = lstm_size * 2        else:            self.linear_size = lstm_size                    self.linear = nn.Linear(self.linear_size, 3)        self.softmax = nn.Softmax(dim = 1)        def forward(self, inputs):        x = self.embeddings(inputs)        x = self.lstm(x)[1][0]        # print(x.shape)        if self.bidirectional:            x = x[-2 :, :, :]            x = x.permute(1,0,2).reshape(x.size(1),-1)        else:            x = x[-1, :, :]            x = x.reshape(x.size(0),-1)        x = self.linear(x)        x = self.softmax(x)        return x        def train(self, train_dataloader, validation_dataset, epochs, loss_func, optimizer):        num_samples = len(train_loader.dataset)        loss_train_list = []        accuracy_train_list = []        loss_validation_list = []        accuracy_validation_list = []        validation_dataset = validation_dataset[:, :].to(device)                for epoch in range(epochs):                correct_sentiments = 0            epoch_loss = 0                        for data in train_dataloader:                data = data.to(device)                review = data[:, :-1]                sentiment_real = data[:, -1]                                sentiment_pred = self.forward(review)                sentiment_pred_args = torch.argmax(sentiment_pred, dim = 1)                correct_sentiments += torch.sum(sentiment_pred_args == sentiment_real).item()                                optimizer.zero_grad()                loss_train = loss_func(sentiment_pred, sentiment_real)                loss_train.backward()                epoch_loss += loss_train.item()                optimizer.step()                        validation_preds = self.forward(validation_dataset[:, :-1])            loss_validatoion = loss_func(validation_preds, validation_dataset[:, -1]).item()            accuracy_validation = (torch.sum(torch.argmax(validation_preds, dim = 1) == validation_dataset[:, -1]).item() / len(validation_dataset))                        loss_train_list.append(epoch_loss/len(train_loader))            accuracy_train_list.append(correct_sentiments/num_samples * 100)            loss_validation_list.append(loss_validatoion)            accuracy_validation_list.append(accuracy_validation * 100)                                    print(f"epoch : {epoch + 1}, training loss : {epoch_loss/len(train_loader):.4f}, training accuracy : {correct_sentiments/num_samples * 100:.1f}")            print(f"epoch : {epoch + 1}, validation loss : {loss_validatoion:.4f}, validation accuracy : {accuracy_validation * 100:.1f}")            print()                    plt.figure()        plt.plot(range(1, epochs + 1), loss_train_list, label = "training loss")        plt.plot(range(1, epochs + 1), loss_validation_list, label = "validation loss")        plt.legend()        plt.xlabel("epochs")        plt.ylabel("loss")                plt.figure()        plt.plot(range(1, epochs + 1), accuracy_train_list, label = "training accuracy")        plt.plot(range(1, epochs + 1), accuracy_validation_list, label = "validatoin accuracy")        plt.legend()        plt.xlabel("epochs")        plt.ylabel("accuracy")            def evaluate(self):        pass    import timeseed = 5start_time = time.time()torch.manual_seed(seed)torch.cuda.manual_seed_all(seed)torch.backends.cudnn.deterministic = Truetorch.backends.cudnn.benchmark = False  # Disable for reproducibilitytorch.cuda.empty_cache()device = torch.device("cuda")# torch.mps.synchronize()# mps_device = 'cpu'# mps_device = torch.device("mps")model = sentiment(len(vocabulary)).to(device)loss_func = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)model.train(train_loader, validation_dataset, 50, loss_func, optimizer)elapsed_time = time.time() - start_timeprint( "Time: ", elapsed_time)model.to("cpu")predictions = torch.argmax(model.forward(validation_dataset[:, :-1]), dim = 1)report = classification_report(validation_dataset[:, -1],predictions, output_dict=True, zero_division=0)