# Importing the required librariesimport torchfrom torch import nnfrom torch.utils.data import DataLoader, Datasetimport matplotlib.pyplot as pltfrom sklearn.metrics import classification_reportimport nltkfrom nltk.tokenize import word_tokenizeimport refrom nltk.stem import WordNetLemmatizernltk.download("punkt")nltk.download('wordnet')import data_preparationfrom data_preparation import datasetimport timefrom datetime import datetimeimport jsonimport os#------------------------------------------------------------------------------# Function Definitions# Defining the Modelclass sentiment(nn.Module):        # Initializing the model    def __init__(self, vocab_len, embed_dim = 20, lstm_size = 20, bidirectional = True, num_layers = 1, dropout = 0):        super().__init__()        # Saving necessary parameters        self.num_layers = num_layers        self.bidirectional = bidirectional        self.training_info = []                # Embedding layer. Expects indices between 0 and vocab_len and generates vectors of embed_dim         # for each index. Also assigns 0 to the padding index vector.        # Input of shape (batch_size, sequence_len)        self.embeddings = nn.Embedding(vocab_len, embed_dim, padding_idx = 0)        # LSTM layer. Expects input of shape (batch_size, sequence_len, embed_dim)        self.lstm = nn.LSTM(embed_dim, hidden_size = lstm_size, batch_first = True, bidirectional = bidirectional, dropout = dropout)                # Determining the size of the linear layer based on the biderectionality of LSTM        if self.bidirectional:            self.linear_size = lstm_size * 2        else:            self.linear_size = lstm_size                # Linear layer. Expects inputs of shape (batch_size, linear_size)        self.linear = nn.Linear(self.linear_size, 1)        # The sigmoid layer        self.sigmoid = nn.Sigmoid()        # Defining the forward pass    def forward(self, inputs):                # Running the input sequence through the embedding layer and lstm layer        x = self.embeddings(inputs)        x = self.lstm(x)[1][0]                # Determining which part of the LSTM output to use based on LSTM structure        if self.bidirectional:            x = x[-2 :, :, :]            x = x.permute(1,0,2).reshape(x.size(1),-1)        else:            x = x[-1, :, :]            x = x.reshape(x.size(0),-1)        # PAssing the LSTM output through a linear and a softmax layer        x = self.linear(x)        return x[:,0]        def train_(self, train_dataloader, validation_dataloader, epochs, loss_func, optimizer, device = torch.device("cpu"),threshold = 0.5):        """        Runs the training loop for of the model and trains the model based on the input parameters.        Parameters        ----------        train_dataloader : obj            PyTorch DataLoader object for training data.        validation_dataset : obj            PyTorch Database object for validation evaulation.        epochs : int            DESCRIPTION.        loss_func : obj            Loss function to use.        optimizer : obj            Optimizer to use.        Returns        -------        None        """        # Saving and defining the required parameters and variables        self.loss_func = loss_func        num_samples = len(train_dataloader.dataset)        loss_train_list = []        accuracy_train_list = []        loss_validation_list = []        accuracy_validation_list = []                        # Running the training loop        for epoch in range(epochs):                correct_sentiments_train = 0            epoch_loss_train = 0            for train_data in train_dataloader:                train_data = train_data.to(device)                # the last elemnt in the sequence is the sentiment and the rest are the input sequence                review = train_data[:, :-1]                sentiment_real_train = train_data[:, -1].to(torch.float)                sentiment_pred_train = self.forward(review)                sentiment_pred_train = self.sigmoid(sentiment_pred_train)                sentiment_pred_train_args = torch.where(sentiment_pred_train >= threshold, torch.tensor(1).to(device), torch.tensor(0).to(device))                correct_sentiments_train += torch.sum(sentiment_pred_train_args == sentiment_real_train).item()                                optimizer.zero_grad()                loss_train = self.loss_func(sentiment_pred_train, sentiment_real_train)                loss_train.backward()                epoch_loss_train += loss_train.item()                optimizer.step()                            loss_train_list.append(epoch_loss_train/len(train_dataloader))            accuracy_train_list.append(correct_sentiments_train/num_samples * 100)                        correct_sentiments_validation = 0            epoch_loss_validation = 0                       with torch.no_grad():                for validation_data in validation_dataloader:                    validation_data = validation_data.to(device)                    sentiment_real_validation = validation_data[:, -1].to(torch.float)                    sentiment_pred_validation = self.forward(validation_data[:, :-1])                    sentiment_pred_validation = self.sigmoid(sentiment_pred_validation)                                        epoch_loss_validation += self.loss_func(sentiment_pred_validation, sentiment_real_validation).item()                                        sentiment_pred_validation_args = torch.where(sentiment_pred_validation >= threshold, torch.tensor(1).to(device), torch.tensor(0).to(device))                    correct_sentiments_validation += torch.sum(sentiment_pred_validation_args == sentiment_real_validation).item()                                loss_validation_list.append(epoch_loss_validation/len(validation_dataloader))            accuracy_validation_list.append(correct_sentiments_validation/len(validation_dataloader.dataset) * 100)                        loss_info = f"epoch : {epoch + 1}, training loss : {loss_train_list[-1]:.4f}, training accuracy : {accuracy_train_list[-1]:.1f}%"            print(loss_info)            self.training_info.append(loss_info)            accuracy_info = f"epoch : {epoch + 1}, validation loss : {loss_validation_list[-1]:.4f}, validation accuracy : {accuracy_validation_list[-1]:.1f}%"            print(accuracy_info)            self.training_info.append(accuracy_info)            print()            self.training_info.append('\n')                 # Plotting the training and validation accuracy and loss during the model training        plt.figure()        plt.plot(range(1, epochs + 1), loss_train_list, label = "training loss")        plt.plot(range(1, epochs + 1), loss_validation_list, label = "validation loss")        plt.legend()        plt.xlabel("epochs")        plt.ylabel("loss")        plt.title("Train and Validation Loss per Epoch")                plt.figure()        plt.plot(range(1, epochs + 1), accuracy_train_list, label = "training accuracy (%)")        plt.plot(range(1, epochs + 1), accuracy_validation_list, label = "validatoin accuracy (%)")        plt.legend()        plt.xlabel("epochs")        plt.ylabel("accuracy")        plt.title("Train and Validation Accuracy per Epoch")            def evaluate(self, test_dataloader, threshold, device):        """        Evaluates the classification of the model based on the test set.         Parameters        ----------        test_dataset : obj            PyTorch test Dataset.        Returns        -------        report : dict            Classification evaluation report.        """        with torch.no_grad():            correct_sentiments_test = 0            loss_test = 0              predictions_arg_list = torch.tensor([]).to(device)            for test_data in test_dataloader:                test_data = test_data[:, :].to(device)                predictions = self.forward(test_data[:, :-1])                predictions_arg = torch.where(predictions >= threshold, torch.tensor(1).to(device), torch.tensor(0).to(device))                predictions_arg_list = torch.cat((predictions_arg_list, predictions_arg), dim = 0)                correct_sentiments_test += torch.sum(predictions_arg == test_data[:, -1]).item()                loss_test += self.loss_func(predictions, test_data[:, -1].to(torch.float)).item()                        report = classification_report(test_dataloader.dataset[:, -1].to("cpu"), predictions_arg_list.to("cpu"), output_dict=True, zero_division=0)            test_info = f"test set loss : {loss_test/len(test_dataloader):.4}, test set accuracy : {correct_sentiments_test/len(test_dataloader.dataset) * 100:.1f}%"            print(test_info)            self.training_info.append(test_info)            return report def set_to_gpu():    """    Sets the device to GPU if available otherwise sets it to CPU. Uses MPS if on mac and CUDA     otherwise.    Returns    -------    device : obj.        PyTorch device object for running the mode on GPU or CPU.    """    # Setting the constant seed for repeatability of results.    seed = 10    torch.manual_seed(seed)        # Setting the device to CUDA if available    if torch.cuda.is_available():       device = torch.device("cuda")       torch.cuda.manual_seed_all(seed)       torch.cuda.empty_cache()    # Setting the device to MPS if available       elif torch.backends.mps.is_available():        device = torch.device("mps")        torch.use_deterministic_algorithms(True)        torch.mps.manual_seed(seed)        torch.mps.empty_cache()    # Setting the device to CPU if GPU not available    else:        device = torch.device("cpu")    # Setting deterministicc behaviour for repatability of results.     torch.backends.cudnn.deterministic = True    torch.backends.cudnn.benchmark = False    return devicedef train_model(embed_dim = 20, lstm_size = 20, bidirectional = True, num_layers = 1, dropout = 0, learning_rate = 0.001, epochs = 100, threshold = 0.5, batch_size = 64):    """    Trains an LSTM model using the input parameters. Saves the model. Also evaluates the     classification accuracy on the test set and return the classification report    besides the model object.    Parameters    ----------    embed_dim : int, optional        Size of the embedding vector for each token. The default is 20.    lstm_size : int, optional        Size of the lstm output. The default is 20.    bidirectional : boolean, optional        Whether to run a bidirectional LSTM. The default is True.    num_layers : int, optional        Number of LSTM layers. The default is 1.    dropout : float, optional        LSTM dropout. The default is 0.    learning_rate : float, optional        Learning rate for trianing the model. The default is 0.001.    epochs : int, optional        Number of epochs to run. The default is 100.    Returns    -------    report : dict        A dictionary contatining the classification report based on the test datset.    model : oobj        PyTorch model.    """    # Starting the timer to measure how long model training takes    start_time = time.time()    # Setting the device to GPU if available    device = set_to_gpu()        # Reading the vocabulary     vocabulary = torch.load("./../data/vocabulary.pth")    # Reading the train, test and validation datasets    training_dataset = torch.load("./../data/training/training_dataset.pth")    validation_dataset = torch.load("./../data/validation/validation_dataset.pth")    test_dataset = torch.load("./../data/test/test_dataset.pth")    # Creating a dataloader from he training dtaset for the model training loop    train_loader = DataLoader(training_dataset, batch_size = batch_size, shuffle = True)        validation_loader = DataLoader(validation_dataset, batch_size = batch_size, shuffle = True)     test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)     # Instansiating the model    model = sentiment(len(vocabulary), embed_dim, lstm_size, bidirectional, num_layers, dropout).to(device)    # Instansiating the optimizer and loss function    loss_func = nn.BCEWithLogitsLoss()    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)    # Training the model    model.train_(train_loader, validation_loader, epochs, loss_func, optimizer, device, threshold)        # Measuring the elapsed time and reporting it    elapsed_time = time.time() - start_time    time_info = f"\nTime it took to train the model: {elapsed_time:.1f}s\n"    print(time_info)    model.training_info.append(time_info)    #Evaluating the model using the test set and saving the classification report    model.eval()    report = model.evaluate(test_loader, threshold, device)        # Defining the name of the model    now = datetime.now()    model_name = "./../models/LSTM model-" + now.strftime("%y_%m_%d-%H_%M_%S")    os.mkdir(model_name)    # Saving the model    torch.save(model, model_name + '/model.pth')    # Saving the classification report    with open(model_name + "/classification_report.json", "w") as file:        json.dump(report, file)        with open(model_name + "/training_info.txt", "w") as file:        file.write("\n".join(model.training_info))        return report, model        #------------------------------------------------------------------------------# Running the script directlyif __name__ == "__main__":        # Size of the embedding vector for each token    embed_dim = 1    # Size of the lstm output    lstm_size = 1    # Whether to run a bidirectional LSTM    bidirectional = False    # Number of LSTM layers    num_layers = 1    # LSTM dropout    dropout = 0    # Learning rate for trianing the model    learning_rate = 0.001    # Number of epochs to run    epochs = 1    # Setting the threshold for positive and negative labels    threshold = 0.5    # Number of batches to use for each parameter update    batch_size = 64        # Training the model    report, model = train_model(embed_dim, lstm_size, bidirectional, num_layers, dropout, learning_rate, epochs, threshold, batch_size)                                                                                    